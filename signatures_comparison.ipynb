{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e27aa",
   "metadata": {},
   "source": [
    "### Selecting the IDs that have all signatures common to the references and sorting them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320c6d8",
   "metadata": {},
   "source": [
    "This directory must contain:\n",
    "- The file common_signatures_per_gene.csv generated by the interproscan_ref_summary.ipynb notebook;\n",
    "- A directory containing the InterProScan output files for the database sequences.\n",
    "\n",
    "The file common_signatures_per_gene.csv was modified to merge lic3A and lic3B into a single classification (lic3X), and to treat lst and cpsK as a single classification (lst). These changes were applied because these proteins share identical InterPro signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af572c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Input paths ===\n",
    "signatures_csv = \"common_signatures_per_gene.csv\"  # your file with relevant signatures\n",
    "interpro_dir = Path(\"./interpro_outputs/\")             # directory with InterProScan .tsv files\n",
    "output_dir = Path(\"./filtered_sequences/\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# === 2. Read the relevant signatures file ===\n",
    "df_signatures = pd.read_csv(signatures_csv)\n",
    "df_signatures['common_signatures'] = df_signatures['common_signatures'].astype(str)\n",
    "\n",
    "# Dictionary: gene -> set of signatures\n",
    "signatures_dict = (\n",
    "    df_signatures.groupby('Gene')['common_signatures']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# === 3. Function to process an InterProScan file ===\n",
    "def read_interproscan_tsv(file):\n",
    "    columns = [\n",
    "        \"seq_id\", \"md5\", \"length\", \"analysis\", \"signature_accession\",\n",
    "        \"signature_desc\", \"start\", \"end\", \"evalue\", \"status\",\n",
    "        \"date\", \"ipr\", \"ipr_desc\", \"go\", \"pathway\"\n",
    "    ]\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names=columns, usecols=[0, 4])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"seq_id\", \"signature_accession\"])\n",
    "\n",
    "# === 4. Combine all InterProScan files ===\n",
    "dfs = []\n",
    "for file in interpro_dir.glob(\"*.tsv\"):\n",
    "    df_tmp = read_interproscan_tsv(file)\n",
    "    dfs.append(df_tmp)\n",
    "\n",
    "df_interpro = pd.concat(dfs, ignore_index=True)\n",
    "df_interpro.dropna(subset=[\"seq_id\", \"signature_accession\"], inplace=True)\n",
    "\n",
    "# === 5. Group signatures by sequence ===\n",
    "signatures_per_seq = (\n",
    "    df_interpro.groupby(\"seq_id\")[\"signature_accession\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# === 6. For each gene, find sequences that contain all signatures ===\n",
    "for gene, reference_signatures in signatures_dict.items():\n",
    "    matching_ids = [\n",
    "        seq_id for seq_id, seq_signatures in signatures_per_seq.items()\n",
    "        if reference_signatures.issubset(seq_signatures)\n",
    "    ]\n",
    "    \n",
    "    # Save result\n",
    "    output_path = output_dir / f\"{gene}_sequences.txt\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for seq_id in matching_ids:\n",
    "            f.write(seq_id + \"\\n\")\n",
    "    \n",
    "    print(f\"âœ… {gene}: {len(matching_ids)} sequences saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317de0a",
   "metadata": {},
   "source": [
    "### Organizing the filtered sequences in a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main paths ===\n",
    "dir_ids = \"./filtered_sequences\"          # where the gene_sequences.txt files are located\n",
    "dir_tsv = \"./interpro_outputs/\"            # where the InterProScan .tsv files are located\n",
    "\n",
    "# List to accumulate the data\n",
    "data = []\n",
    "\n",
    "# === 1. Load all InterProScan outputs ===\n",
    "# Read all TSVs into a single DataFrame\n",
    "df_list = []\n",
    "for file in os.listdir(dir_tsv):\n",
    "    if file.endswith(\".tsv\"):\n",
    "        path = os.path.join(dir_tsv, file)\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str)\n",
    "            df_tmp = df_tmp[[0, 4]]  # Column 0 = seq_id, Column 4 = signature (IPRxxxxx)\n",
    "            df_tmp.columns = [\"seq_id\", \"signature\"]\n",
    "            df_list.append(df_tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error reading {file}: {e}\")\n",
    "\n",
    "# Concatenate all InterProScan outputs into a single DataFrame\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"âœ… .tsv files combined: {len(df_all)} total rows\")\n",
    "\n",
    "# === 2. For each gene, retrieve the signatures ===\n",
    "for file in os.listdir(dir_ids):\n",
    "    if file.endswith(\"_sequences.txt\"):\n",
    "        gene = file.split(\"_sequences.txt\")[0]\n",
    "        ids_path = os.path.join(dir_ids, file)\n",
    "\n",
    "        # Read sequence IDs (one per line)\n",
    "        with open(ids_path) as f:\n",
    "            ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        # Filter the large dataframe by the sequences of this gene\n",
    "        df_filtered = df_all[df_all[\"seq_id\"].isin(ids)].copy()\n",
    "\n",
    "        # Add gene column\n",
    "        df_filtered[\"gene\"] = gene\n",
    "\n",
    "        data.append(df_filtered)\n",
    "\n",
    "        print(f\"ðŸ”¹ {gene}: {len(df_filtered)} signatures found for {len(ids)} sequences\")\n",
    "\n",
    "# === 3. Merge everything and save ===\n",
    "df_final = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Reorder columns\n",
    "df_final = df_final[[\"seq_id\", \"gene\", \"signature\"]]\n",
    "\n",
    "# Show first rows\n",
    "print(df_final.head())\n",
    "\n",
    "# === 4. (Optional) save to CSV ===\n",
    "df_final.to_csv(\"filtered_seqs_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead10bf8",
   "metadata": {},
   "source": [
    "### Checking which sequences have signatures other than the reference ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Read files ===\n",
    "df_all = pd.read_csv(\"./filtered_seqs_summary.csv\")\n",
    "df_ref = pd.read_csv(\"./common_signatures_per_gene.csv\")\n",
    "\n",
    "# === 2. Ensure consistency and cleaning ===\n",
    "# Clean column names\n",
    "df_ref.columns = df_ref.columns.str.strip()\n",
    "df_all.columns = df_all.columns.str.strip()\n",
    "\n",
    "# Normalize column contents\n",
    "for col in [\"gene\", \"signature\"]:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = (\n",
    "            df_all[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace(\"\\r\", \"\", regex=False)\n",
    "            .str.replace(\"\\n\", \"\", regex=False)\n",
    "            .str.upper()\n",
    "        )\n",
    "\n",
    "if \"Gene\" in df_ref.columns:\n",
    "    df_ref[\"Gene\"] = (\n",
    "        df_ref[\"Gene\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"\\r\", \"\", regex=False)\n",
    "        .str.replace(\"\\n\", \"\", regex=False)\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "# Convert reference signatures into lists\n",
    "df_ref[\"common_signatures\"] = df_ref[\"common_signatures\"].apply(\n",
    "    lambda x: [s.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").upper() for s in str(x).split(\";\") if s.strip()]\n",
    ")\n",
    "\n",
    "# === 3. Group signatures by sequence ===\n",
    "seq_signatures = (\n",
    "    df_all.groupby([\"seq_id\", \"gene\"])[\"signature\"]\n",
    "    .apply(lambda x: set(s.strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").upper() for s in x if s))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# === 4. Compare with the reference set ===\n",
    "results = []\n",
    "\n",
    "for _, row in seq_signatures.iterrows():\n",
    "    seq_id = row[\"seq_id\"]\n",
    "    gene = row[\"gene\"]\n",
    "    sigs = row[\"signature\"]\n",
    "\n",
    "    # Reference rows for the same gene\n",
    "    ref_row = df_ref[df_ref[\"Gene\"] == gene]\n",
    "    if ref_row.empty:\n",
    "        continue  # skip if the gene has no reference\n",
    "\n",
    "    # Merge all reference signature lists\n",
    "    ref_sigs = set().union(*ref_row[\"common_signatures\"])\n",
    "\n",
    "    # Check if it contains all reference signatures AND has extras\n",
    "    if ref_sigs.issubset(sigs) and len(sigs - ref_sigs) > 0:\n",
    "        extra = sigs - ref_sigs\n",
    "        results.append({\n",
    "            \"seq_id\": seq_id,\n",
    "            \"gene\": gene,\n",
    "            \"extra_signatures\": \";\".join(sorted(extra))\n",
    "        })\n",
    "\n",
    "# === 5. Save results ===\n",
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(\"seqs_with_extra_signatures.csv\", index=False)\n",
    "\n",
    "print(f\"{len(df_result)} sequences have additional signatures.\")\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c227be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE HELPED ME DURING CURATION â€“ BUT I NO LONGER USE IT\n",
    "\n",
    "# Read the original output file\n",
    "df = pd.read_csv(\"seqs_with_extra_signatures.csv\")\n",
    "\n",
    "# === Text normalization ===\n",
    "df[\"gene\"] = df[\"gene\"].astype(str).str.strip().str.upper()\n",
    "df[\"extra_signatures\"] = (\n",
    "    df[\"extra_signatures\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace(\"\\r\", \"\", regex=False)\n",
    "    .str.replace(\"\\n\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# === Removal conditions ===\n",
    "\n",
    "# 1ï¸âƒ£ Remove COIL / MOBIDB-LITE (alone or together, any order)\n",
    "cond1 = df[\"extra_signatures\"].isin([\n",
    "    \"COIL\",\n",
    "    \"MOBIDB-LITE\",\n",
    "    \"COIL;MOBIDB-LITE\",\n",
    "    \"MOBIDB-LITE;COIL\"\n",
    "])\n",
    "\n",
    "# 2ï¸âƒ£ Remove specific G3DSA when gene == LST\n",
    "cond2 = (df[\"gene\"] == \"LST\") & df[\"extra_signatures\"].isin([\n",
    "    \"G3DSA:3.40.50.1110\",\n",
    "    \"G3DSA:3.30.370.20\"\n",
    "])\n",
    "\n",
    "# 3ï¸âƒ£ Remove specific G3DSA when gene == LIC3X\n",
    "cond3 = (df[\"gene\"] == \"LIC3X\") & (df[\"extra_signatures\"] == \"G3DSA:3.90.1480.10\")\n",
    "\n",
    "# Remove PF13472 (GDSL-like Lipase/Acylhydrolase family)\n",
    "cond4 = (df[\"gene\"] == \"NEUA\") & df[\"extra_signatures\"].isin([\"PF13472\"])\n",
    "\n",
    "# Remove SSF53756 (UDP-Glycosyltransferase)\n",
    "cond5 = df[\"extra_signatures\"].str.contains(r\"\\bSSF53756\\b\", na=False, regex=True)\n",
    "\n",
    "# Remove PS51257 (lipoprotein lipid attachment site profile)\n",
    "cond6 = df[\"extra_signatures\"].str.contains(r\"\\bPS51257\\b\", na=False, regex=True)\n",
    "\n",
    "# Remove SSF52266 (SGNH hydrolases)\n",
    "cond7 = df[\"extra_signatures\"].str.contains(r\"\\bSSF52266\\b\", na=False, regex=True)\n",
    "\n",
    "# Remove PF00535 (Glycosyl transferase family 2)\n",
    "cond8 = df[\"extra_signatures\"].str.contains(r\"\\bPF00535\\b\", na=False, regex=True)\n",
    "\n",
    "# Remove PF01670\n",
    "cond9 = df[\"extra_signatures\"].str.contains(r\"\\bPF01670\\b\", na=False, regex=True)\n",
    "\n",
    "# Remove CD00761\n",
    "cond10 = df[\"extra_signatures\"].str.contains(r\"\\bCD00761\\b\", na=False, regex=True)\n",
    "\n",
    "# Combine everything\n",
    "cond_total = cond1 | cond2 | cond3 | cond4 | cond5 | cond6 | cond7 | cond8 | cond9 | cond10\n",
    "\n",
    "# === Apply filter ===\n",
    "df_filtered = df[~cond_total]\n",
    "\n",
    "# === Save result ===\n",
    "df_filtered.to_csv(\"seqs_with_extra_signatures_filtered.csv\", index=False)\n",
    "\n",
    "# === Report ===\n",
    "n_removed = len(df) - len(df_filtered)\n",
    "print(f\"{n_removed} rows were removed.\")\n",
    "print(f\"{len(df_filtered)} remaining rows saved to 'seqs_with_extra_signatures_filtered.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348d07e",
   "metadata": {},
   "source": [
    "### Taking the dataframe seqs_with_extra_signatures.csv and filtering what will definitely be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Read original file ===\n",
    "file = \"seqs_with_extra_signatures.csv\"\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# === 2. Text normalization ===\n",
    "df[\"gene\"] = df[\"gene\"].astype(str).str.strip().str.upper()\n",
    "df[\"extra_signatures\"] = (\n",
    "    df[\"extra_signatures\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    ")\n",
    "\n",
    "# === 3. Define lists and conditions ===\n",
    "\n",
    "signatures_to_remove = [\n",
    "    \"PF15632\", \"PF00903\", \"PF01973\", \"PF01370\", \"PF02826\", \"PF07883\", \"PF01029\",\n",
    "    \"PF13411\", \"PF22725\", \"PF01408\", \"PF03737\", \"PF13561\", \"PF00389\", \"PF17836\",\n",
    "    \"PF00005\", \"PF03102\", \"PF00483\", \"PF04545\", \"PF08240\", \"PF09861\", \"PF02558\",\n",
    "    \"PF01583\", \"PS50007\", \"PS50008\", \"PS50175\", \"PF00459\", \"PF02737\", \"PF04932\",\n",
    "    \"PS00126\", \"PS01319\", \"PS50890\", \"PS50893\", \"SSF51161\", \"CD04647\", \"PTHR36489\", \n",
    "    \"PTHR42781\", \"TIGR03584\", \"PF08282\", \"PF22352\", \"SSF56784\", \"PF06306\", \"SSF53335\", \n",
    "    \"PF01895\", \"SSF109755\", \"PS00211\", \"SSF52540\", \"PR01179\", \"SSF50621\", \"PF02784\", \n",
    "    \"SSF51419\", \"SFLDS00003\", \"SFLDG01136\", \"SFLDG01138\", \"TIGR01670\", \"CD01630\",\n",
    "    \"PF00535\", \"SSF53448\", \"PF04101\", \"PF01670\", \"PF13302\", \"PF04101\",\n",
    "    \"PF01501\", \"CD03801\", \"G3DSA:3.40.50.2000\", \"PF00571\", \"PF02350\", \n",
    "    \"PF14602\", \"CD10144\", \"PF14602\", \"PF08889\", \"SSF51735\", \"PF00364\", \n",
    "    \"PF15632\", \"SSF53335\", \"SSF51230\", \"PF13439\", \"SSF48452\", \"PS50287\",\n",
    "    \"PF13946\", \"PF12895\", \"SSF81483\", \"TIGR03028\", \"SSF142984\", \"PF06251\",\n",
    "    \"PS51257\", \"PS51318\", \"G3DSA:3.10.20.600\", \"PF01061\", \"PS00858\",      \n",
    "    \"PF00535\", \"PS00430\", \"PS50180\", \"PF01469\", \"PS50889\", \"PS50832\",\n",
    "    \"PF03448\", \"CD22265\", \"SSF103657\", \"PF13469\", \"PF05686\", \"SSF56784\",\n",
    "    \"PF02706\", \"PR01868\", \"CD06530\", \"PS50096\", \"PF00756\", \"PF04230\",\n",
    "    \"G3DSA:1.10.3130.20\", \"PS00307\", \"G3DSA:2.70.50.60\", \"G3DSA:1.20.5.340\", \n",
    "    \"PF01041\", \"PS51191\", \"PF04339\", \"SSF57997\"   \n",
    "]\n",
    "\n",
    "# Helper function â€” converts signatures into a list and checks for presence\n",
    "def has_inadequate_signature(signatures, forbidden):\n",
    "    sig_list = [s.strip() for s in str(signatures).split(\";\")]\n",
    "    return any(sig in sig_list for sig in forbidden)\n",
    "\n",
    "# cond1: remove any row that contains at least one forbidden signature\n",
    "cond1 = df[\"extra_signatures\"].apply(lambda x: has_inadequate_signature(x, signatures_to_remove))\n",
    "\n",
    "# cond2: neuA or neuS â†’ remove if it contains sialyltransferase signatures\n",
    "cond2 = (\n",
    "    df[\"gene\"].isin([\"NEUA\", \"NEUS\"]) &\n",
    "    df[\"extra_signatures\"].apply(lambda x: has_inadequate_signature(x, [\"PF07922\", \"SSF102414\", \"PF11477\"]))\n",
    ")\n",
    "\n",
    "# cond3: neuA â†’ remove if it contains neuS signatures\n",
    "cond3 = (\n",
    "    (df[\"gene\"] == \"NEUA\") &\n",
    "    df[\"extra_signatures\"].apply(lambda x: has_inadequate_signature(x, [\"PF07388\"]))\n",
    ")\n",
    "\n",
    "# cond4: neuS â†’ remove if it contains neuA signatures\n",
    "cond4 = (\n",
    "    (df[\"gene\"] == \"NEUS\") &\n",
    "    df[\"extra_signatures\"].apply(lambda x: has_inadequate_signature(x, [\"G3DSA:3.90.550.10\", \"SSF53448\"]))\n",
    ")\n",
    "\n",
    "# cond5: LIC3X, LST, PM0188 â†’ remove neuA/neuS signatures\n",
    "cond5 = (\n",
    "    df[\"gene\"].isin([\"LIC3X\", \"LST\", \"PM0188\"]) &\n",
    "    df[\"extra_signatures\"].apply(lambda x: has_inadequate_signature(x, [\"PF07388\", \"G3DSA:3.90.550.10\"]))\n",
    ")\n",
    "\n",
    "# === 4. Combine conditions and filter ===\n",
    "cond_total = cond1 | cond2 | cond3 | cond4 | cond5\n",
    "\n",
    "sequences_to_exclude = df[cond_total].copy()\n",
    "df_filtered = df[~cond_total].copy()\n",
    "\n",
    "# === 5. Save results ===\n",
    "sequences_to_exclude.to_csv(\"sequences_to_exclude.csv\", index=False)\n",
    "df_filtered.to_csv(\"seqs_with_extra_signatures_filtered.csv\", index=False)\n",
    "\n",
    "# === 6. Report ===\n",
    "print(f\"ðŸ”¹ Original rows: {len(df)}\")\n",
    "print(f\"ðŸ”¸ Sequences to exclude: {len(sequences_to_exclude)}\")\n",
    "print(f\"âœ… Remaining rows after curation: {len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0fe70",
   "metadata": {},
   "source": [
    "### Collecting IDs to be deleted (from sequences that have all signatures common to the references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef2ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "df = pd.read_csv(\"sequences_to_exclude.csv\", sep=\",\") \n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"exclude\", exist_ok=True)\n",
    "\n",
    "# Iterate over unique genes\n",
    "for gene, grupo in df.groupby(\"gene\"):\n",
    "    # Get all seq_id associated with this gene\n",
    "    seq_ids = grupo[\"seq_id\"].tolist()\n",
    "    \n",
    "    # Define the output file name\n",
    "    output_file = f\"exclude/{gene}_exclude_from_filtered_sequences.txt\"\n",
    "    \n",
    "    # Save one ID per line\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(seq_ids))\n",
    "    \n",
    "    print(f\"File created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e558aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the files are located\n",
    "pasta = \"./exclude\"  \n",
    "\n",
    "# Pattern to find the files\n",
    "files = glob.glob(os.path.join(pasta, \"*.txt\"))\n",
    "\n",
    "# Output file path (in the same folder)\n",
    "output_file = os.path.join(pasta, \"concatenated.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as output:\n",
    "    for name in files:\n",
    "        with open(name, \"r\") as f:\n",
    "            content = f.read()\n",
    "            output.write(content)\n",
    "            output.write(\"\\n\")  \n",
    "\n",
    "print(f\"Concatenated file saved at:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841eb37",
   "metadata": {},
   "source": [
    "The IDs to be deleted must be concatenated, and the code to remove them must search for all IDs in all files for the different genes, since they have been mixed together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f85be",
   "metadata": {},
   "source": [
    "### Excluding inappropriate IDs from filtered sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Folder paths and exclusion file ===\n",
    "seq_folder = \"filtered_sequences\"\n",
    "output_folder = \"filtered_sequences_cleaned\"\n",
    "exclude_file = \"./exclude/concatenated.txt\"  # your concatenated file with all IDs\n",
    "\n",
    "# === Create output folder ===\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === Read IDs to exclude ===\n",
    "with open(exclude_file) as f:\n",
    "    ids_to_exclude = set(l.strip() for l in f if l.strip())\n",
    "\n",
    "print(f\"Total IDs to exclude: {len(ids_to_exclude)}\")\n",
    "\n",
    "# === Process all files in the folder ===\n",
    "for file in os.listdir(seq_folder):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(seq_folder, file)\n",
    "    output_path = os.path.join(output_folder, file)\n",
    "\n",
    "    with open(input_path) as f:\n",
    "        lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "    # Remove any line whose ID is in the exclusion list\n",
    "    filtered_lines = [l for l in lines if l not in ids_to_exclude]\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(filtered_lines))\n",
    "\n",
    "    print(f\"Clean file saved: {output_path}\")\n",
    "\n",
    "print(\"\\nâœ… All files were processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca374a0",
   "metadata": {},
   "source": [
    "### Extracting FASTA sequences for filtered clean IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7d208",
   "metadata": {},
   "source": [
    "The database FASTA files must be located in a subdirectory of this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "fasta_folder = \"original_merged_fastas\"\n",
    "ids_folder = \"filtered_sequences_cleaned\"\n",
    "output_folder = \"filtered_fastas\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Index all original sequences by ID\n",
    "# -------------------------------\n",
    "seq_dict = {}\n",
    "\n",
    "for fasta_file in os.listdir(fasta_folder):\n",
    "    if not fasta_file.endswith((\".fasta\", \".fa\", \".faa\")):\n",
    "        continue\n",
    "\n",
    "    fasta_path = os.path.join(fasta_folder, fasta_file)\n",
    "\n",
    "    for seq in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq_dict[seq.id] = seq  # ID -> SeqRecord object\n",
    "\n",
    "print(f\"âœ… Total indexed sequences: {len(seq_dict)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Generate FASTAs per gene\n",
    "# -------------------------------\n",
    "for ids_file in os.listdir(ids_folder):\n",
    "    if not ids_file.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    gene = ids_file.split(\"_sequences\")[0]\n",
    "    ids_path = os.path.join(ids_folder, ids_file)\n",
    "\n",
    "    # Read IDs to keep\n",
    "    with open(ids_path) as f:\n",
    "        kept_ids = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "    # Collect corresponding sequences\n",
    "    filtered_seqs = [seq_dict[i] for i in kept_ids if i in seq_dict]\n",
    "\n",
    "    # Output file\n",
    "    output_fasta = os.path.join(output_folder, f\"{gene}_filtered.fasta\")\n",
    "\n",
    "    # Write FASTA\n",
    "    SeqIO.write(filtered_seqs, output_fasta, \"fasta\")\n",
    "\n",
    "    print(f\"âœ… {gene}: {len(filtered_seqs)} sequences saved to {output_fasta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
